{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers4rec[nvtabular] protobuf==4.25.0 pyarrow==12.0.1 pandas-gbq==0.19.2 tritonclient"
      ],
      "metadata": {
        "id": "RrppOj5Y63Gi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d4a152-6933-4b37-82d9-614491da3930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.4/294.4 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.5/485.5 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.5/284.5 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.1/311.1 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.9/994.9 kB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for merlin-models (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers4rec (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for merlin-core (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for merlin-dataloader (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for betterproto (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grpclib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for stringcase (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q cudf-cu11 dask-cudf-cu11 --extra-index-url=https://pypi.nvidia.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjTYou3sO7o5",
        "outputId": "c1bb7f57-5848-4b5d-84e1-a83a9924bef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.6/502.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.7/18.7 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.6/89.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.4/582.4 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts6za2BC6UX4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import calendar\n",
        "import datetime\n",
        "\n",
        "import cudf\n",
        "import cupy\n",
        "import nvtabular as nvt\n",
        "from merlin.dag import ColumnSelector\n",
        "from merlin.schema import Schema, Tags"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import config\n",
        "config.CUDA_LOW_OCCUPANCY_WARNINGS = 0"
      ],
      "metadata": {
        "id": "xr2lOmD26vU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/content/drive/MyDrive/Datasets/rsc15/raw\")\n",
        "FILENAME_PATTERN = 'rsc15-clicks.dat'\n",
        "DATA_PATH = os.path.join(DATA_FOLDER, FILENAME_PATTERN)\n",
        "\n",
        "OUTPUT_FOLDER = \"./yoochoose_transformed\"\n",
        "!mkdir {OUTPUT_FOLDER}\n",
        "OVERWRITE = False\n",
        "\n",
        "USE_SYNTHETIC = os.environ.get(\"USE_SYNTHETIC\", False)"
      ],
      "metadata": {
        "id": "WhlbtpFo8ZlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_data(\n",
        "    start_date: datetime.date, end_date: datetime.date, rows_per_day: int = 10000\n",
        ") -> pd.DataFrame:\n",
        "    assert end_date > start_date, \"end_date must be later than start_date\"\n",
        "\n",
        "    number_of_days = (end_date - start_date).days\n",
        "    total_number_of_rows = number_of_days * rows_per_day\n",
        "\n",
        "    # Generate a long-tail distribution of item interactions. This simulates that some items are\n",
        "    # more popular than others.\n",
        "    long_tailed_item_distribution = np.clip(\n",
        "        np.random.lognormal(3.0, 1.0, total_number_of_rows).astype(np.int64), 1, 50000\n",
        "    )\n",
        "\n",
        "    # generate random item interaction features\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"session_id\": np.random.randint(70000, 80000, total_number_of_rows),\n",
        "            \"item_id\": long_tailed_item_distribution,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # generate category mapping for each item-id\n",
        "    df[\"category\"] = pd.cut(df[\"item_id\"], bins=334, labels=np.arange(1, 335)).astype(\n",
        "        np.int64\n",
        "    )\n",
        "\n",
        "    max_session_length = 60 * 60  # 1 hour\n",
        "\n",
        "    def add_timestamp_to_session(session: pd.DataFrame):\n",
        "        random_start_date_and_time = calendar.timegm(\n",
        "            (\n",
        "                start_date\n",
        "                # Add day offset from start_date\n",
        "                + datetime.timedelta(days=np.random.randint(0, number_of_days))\n",
        "                # Add time offset within the random day\n",
        "                + datetime.timedelta(seconds=np.random.randint(0, 86_400))\n",
        "            ).timetuple()\n",
        "        )\n",
        "        session[\"timestamp\"] = random_start_date_and_time + np.clip(\n",
        "            np.random.lognormal(3.0, 1.0, len(session)).astype(np.int64),\n",
        "            0,\n",
        "            max_session_length,\n",
        "        )\n",
        "        return session\n",
        "\n",
        "    df = df.groupby(\"session_id\").apply(add_timestamp_to_session).reset_index()\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "sjUmWLZdeqJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_SYNTHETIC:\n",
        "    START_DATE = os.environ.get(\"START_DATE\", \"2014/4/1\")\n",
        "    END_DATE = os.environ.get(\"END_DATE\", \"2014/4/5\")\n",
        "    interactions_df = generate_synthetic_data(datetime.datetime.strptime(START_DATE, '%Y/%m/%d'),\n",
        "                                              datetime.datetime.strptime(END_DATE, '%Y/%m/%d'))\n",
        "    interactions_df = cudf.from_pandas(interactions_df)\n",
        "else:\n",
        "    interactions_df = cudf.read_csv(DATA_PATH, sep=',',\n",
        "                                    names=['session_id','timestamp', 'item_id', 'category'],\n",
        "                                    dtype=['int', 'datetime64[s]', 'int', 'int'])"
      ],
      "metadata": {
        "id": "LW24eWhxesYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Count with in-session repeated interactions: {}\".format(len(interactions_df)))\n",
        "\n",
        "# Sorts the dataframe by session and timestamp, to remove consecutive repetitions\n",
        "interactions_df.timestamp = interactions_df.timestamp.astype(int)\n",
        "interactions_df = interactions_df.sort_values(['session_id', 'timestamp'])\n",
        "past_ids = interactions_df['item_id'].shift(1).fillna()\n",
        "session_past_ids = interactions_df['session_id'].shift(1).fillna()\n",
        "\n",
        "# Keeping only no consecutive repeated in session interactions\n",
        "interactions_df = interactions_df[~((interactions_df['session_id'] == session_past_ids) & (interactions_df['item_id'] == past_ids))]\n",
        "\n",
        "print(\"Count after removed in-session repeated interactions: {}\".format(len(interactions_df)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSD15Q-det_0",
        "outputId": "923b81f3-c6c3-49e0-b99f-d75a46ca162d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count with in-session repeated interactions: 33003944\n",
            "Count after removed in-session repeated interactions: 28971543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "items_first_ts_df = interactions_df.groupby('item_id').agg({'timestamp': 'min'}).reset_index().rename(columns={'timestamp': 'itemid_ts_first'})\n",
        "interactions_merged_df = interactions_df.merge(items_first_ts_df, on=['item_id'], how='left')\n",
        "print(interactions_merged_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EZsK8t7ex8o",
        "outputId": "0339b304-9c65-4c2d-bde0-a5b73d42b1d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   session_id   timestamp    item_id  category  itemid_ts_first\n",
            "0        4993  1396727816  214835285         0       1396332436\n",
            "1        4993  1396727863  214530703         0       1396339114\n",
            "2        4993  1396727898  214530705         0       1396330224\n",
            "3        4993  1396728063  214835713         0       1396327474\n",
            "4        4993  1396730097  214512611         0       1396328044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isdir(DATA_FOLDER) == False:\n",
        "    os.mkdir(DATA_FOLDER)\n",
        "interactions_merged_df.to_parquet(os.path.join(DATA_FOLDER, 'interactions_merged_df.parquet'))"
      ],
      "metadata": {
        "id": "cy4tOn_aez17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print the total number of unique items in the dataset\n",
        "print(interactions_merged_df.item_id.nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS90FTQGe8a4",
        "outputId": "b385a012-108a-4f79-c4f2-88199942d923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# free gpu memory\n",
        "del interactions_df, session_past_ids, items_first_ts_df\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vilalZiJfEXs",
        "outputId": "b54cd33e-5331-4049-e11f-de81099004ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encodes categorical features as contiguous integers\n",
        "cat_feats = ColumnSelector(['session_id', 'category', 'item_id']) >> nvt.ops.Categorify()\n",
        "\n",
        "# create time features\n",
        "session_ts = ColumnSelector(['timestamp'])\n",
        "session_time = (\n",
        "    session_ts >>\n",
        "    nvt.ops.LambdaOp(lambda col: cudf.to_datetime(col, unit='s')) >>\n",
        "    nvt.ops.Rename(name = 'event_time_dt')\n",
        ")\n",
        "sessiontime_weekday = (\n",
        "    session_time >>\n",
        "    nvt.ops.LambdaOp(lambda col: col.dt.weekday) >>\n",
        "    nvt.ops.Rename(name ='et_dayofweek')\n",
        ")\n",
        "\n",
        "# Derive cyclical features: Define a custom lambda function\n",
        "def get_cycled_feature_value_sin(col, max_value):\n",
        "    value_scaled = (col + 0.000001) / max_value\n",
        "    value_sin = np.sin(2*np.pi*value_scaled)\n",
        "    return value_sin\n",
        "\n",
        "weekday_sin = sessiontime_weekday >> (lambda col: get_cycled_feature_value_sin(col+1, 7)) >> nvt.ops.Rename(name = 'et_dayofweek_sin')\n",
        "\n",
        "# Compute Item recency: Define a custom Op\n",
        "class ItemRecency(nvt.ops.Operator):\n",
        "    def transform(self, columns, gdf):\n",
        "        for column in columns.names:\n",
        "            col = gdf[column]\n",
        "            item_first_timestamp = gdf['itemid_ts_first']\n",
        "            delta_days = (col - item_first_timestamp) / (60*60*24)\n",
        "            gdf[column + \"_age_days\"] = delta_days * (delta_days >=0)\n",
        "        return gdf\n",
        "\n",
        "    def compute_selector(\n",
        "        self,\n",
        "        input_schema: Schema,\n",
        "        selector: ColumnSelector,\n",
        "        parents_selector: ColumnSelector,\n",
        "        dependencies_selector: ColumnSelector,\n",
        "    ) -> ColumnSelector:\n",
        "        self._validate_matching_cols(input_schema, parents_selector, \"computing input selector\")\n",
        "        return parents_selector\n",
        "\n",
        "    def column_mapping(self, col_selector):\n",
        "        column_mapping = {}\n",
        "        for col_name in col_selector.names:\n",
        "            column_mapping[col_name + \"_age_days\"] = [col_name]\n",
        "        return column_mapping\n",
        "\n",
        "    @property\n",
        "    def dependencies(self):\n",
        "        return [\"itemid_ts_first\"]\n",
        "\n",
        "    @property\n",
        "    def output_dtype(self):\n",
        "        return np.float64\n",
        "\n",
        "recency_features = session_ts >> ItemRecency()\n",
        "# Apply standardization to this continuous feature\n",
        "recency_features_norm = recency_features >> nvt.ops.LogOp() >> nvt.ops.Normalize(out_dtype=np.float32) >> nvt.ops.Rename(name='product_recency_days_log_norm')\n",
        "\n",
        "time_features = (\n",
        "    session_time +\n",
        "    sessiontime_weekday +\n",
        "    weekday_sin +\n",
        "    recency_features_norm\n",
        ")\n",
        "\n",
        "features = ColumnSelector(['session_id', 'timestamp']) + cat_feats + time_features"
      ],
      "metadata": {
        "id": "m0z7u_7sfFoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Groupby Operator\n",
        "groupby_features = features >> nvt.ops.Groupby(\n",
        "    groupby_cols=[\"session_id\"],\n",
        "    sort_cols=[\"timestamp\"],\n",
        "    aggs={\n",
        "        'item_id': [\"list\", \"count\"],\n",
        "        'category': [\"list\"],\n",
        "        'timestamp': [\"first\"],\n",
        "        'event_time_dt': [\"first\"],\n",
        "        'et_dayofweek_sin': [\"list\"],\n",
        "        'product_recency_days_log_norm': [\"list\"]\n",
        "        },\n",
        "    name_sep=\"-\")\n",
        "\n",
        "# Truncate sequence features to first interacted 20 items\n",
        "SESSIONS_MAX_LENGTH = 20\n",
        "\n",
        "item_feat = groupby_features['item_id-list'] >> nvt.ops.TagAsItemID()\n",
        "cont_feats = groupby_features['et_dayofweek_sin-list', 'product_recency_days_log_norm-list'] >> nvt.ops.AddMetadata(tags=[Tags.CONTINUOUS])\n",
        "\n",
        "\n",
        "groupby_features_list =  item_feat + cont_feats + groupby_features['category-list']\n",
        "groupby_features_truncated = groupby_features_list >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH)\n",
        "\n",
        "# Calculate session day index based on 'event_time_dt-first' column\n",
        "day_index = ((groupby_features['event_time_dt-first'])  >>\n",
        "             nvt.ops.LambdaOp(lambda col: (col - col.min()).dt.days +1) >>\n",
        "             nvt.ops.Rename(f = lambda col: \"day_index\") >>\n",
        "             nvt.ops.AddMetadata(tags=[Tags.CATEGORICAL])\n",
        "            )\n",
        "\n",
        "# tag session_id column for serving with legacy api\n",
        "sess_id = groupby_features['session_id'] >> nvt.ops.AddMetadata(tags=[Tags.CATEGORICAL])\n",
        "\n",
        "# Select features for training\n",
        "selected_features = sess_id + groupby_features['item_id-count'] + groupby_features_truncated + day_index\n",
        "\n",
        "# Filter out sessions with less than 2 interactions\n",
        "MINIMUM_SESSION_LENGTH = 2\n",
        "filtered_sessions = selected_features >> nvt.ops.Filter(f=lambda df: df[\"item_id-count\"] >= MINIMUM_SESSION_LENGTH)"
      ],
      "metadata": {
        "id": "QTto5_KYfIbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import config\n",
        "config.CUDA_LOW_OCCUPANCY_WARNINGS = 0"
      ],
      "metadata": {
        "id": "4tOLsCd-vFzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = nvt.Dataset(interactions_merged_df)\n",
        "workflow = nvt.Workflow(filtered_sessions)\n",
        "# Learns features statistics necessary of the preprocessing workflow\n",
        "workflow.fit(dataset)\n",
        "# Apply the preprocessing workflow in the dataset and converts the resulting Dask cudf dataframe to a cudf dataframe\n",
        "sessions_gdf = workflow.transform(dataset).compute()"
      ],
      "metadata": {
        "id": "VMhkBse0vNQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sessions_gdf.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "Zs7rhQ7XvQi0",
        "outputId": "dbd3111e-2b9b-45d2-b347-82582c4cd0da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   session_id  item_id-count                   item_id-list  \\\n",
              "0           1              3           [7342, 19525, 15501]   \n",
              "1           2              5  [1970, 8041, 5180, 9412, 607]   \n",
              "2           3              3               [83, 2163, 2344]   \n",
              "3           4              2                   [2465, 5662]   \n",
              "4           6              2                    [3214, 208]   \n",
              "\n",
              "                               et_dayofweek_sin-list  \\\n",
              "0  [0.7818320421108522, 0.7818320421108522, 0.781...   \n",
              "1  [0.7818320421108522, 0.7818320421108522, 0.781...   \n",
              "2  [0.43388293040961884, 0.43388293040961884, 0.4...   \n",
              "3           [0.7818320421108522, 0.7818320421108522]   \n",
              "4     [8.975979006501142e-07, 8.975979006501142e-07]   \n",
              "\n",
              "                  product_recency_days_log_norm-list    category-list  \\\n",
              "0                [-1.2986504, -1.3283174, -1.354854]        [3, 3, 3]   \n",
              "1  [-1.3023797, -2.2679935, -2.17633, -2.1693697,...  [3, 3, 3, 3, 3]   \n",
              "2               [-2.2485673, -2.3008614, -2.2673779]        [3, 3, 3]   \n",
              "3                            [-1.283754, -1.2788646]           [3, 3]   \n",
              "4                           [-1.3856947, -1.3733647]           [3, 3]   \n",
              "\n",
              "   day_index  \n",
              "0          7  \n",
              "1          7  \n",
              "2          2  \n",
              "3          7  \n",
              "4          6  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>session_id</th>\n",
              "      <th>item_id-count</th>\n",
              "      <th>item_id-list</th>\n",
              "      <th>et_dayofweek_sin-list</th>\n",
              "      <th>product_recency_days_log_norm-list</th>\n",
              "      <th>category-list</th>\n",
              "      <th>day_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>[7342, 19525, 15501]</td>\n",
              "      <td>[0.7818320421108522, 0.7818320421108522, 0.781...</td>\n",
              "      <td>[-1.2986504, -1.3283174, -1.354854]</td>\n",
              "      <td>[3, 3, 3]</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>[1970, 8041, 5180, 9412, 607]</td>\n",
              "      <td>[0.7818320421108522, 0.7818320421108522, 0.781...</td>\n",
              "      <td>[-1.3023797, -2.2679935, -2.17633, -2.1693697,...</td>\n",
              "      <td>[3, 3, 3, 3, 3]</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>[83, 2163, 2344]</td>\n",
              "      <td>[0.43388293040961884, 0.43388293040961884, 0.4...</td>\n",
              "      <td>[-2.2485673, -2.3008614, -2.2673779]</td>\n",
              "      <td>[3, 3, 3]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>[2465, 5662]</td>\n",
              "      <td>[0.7818320421108522, 0.7818320421108522]</td>\n",
              "      <td>[-1.283754, -1.2788646]</td>\n",
              "      <td>[3, 3]</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>[3214, 208]</td>\n",
              "      <td>[8.975979006501142e-07, 8.975979006501142e-07]</td>\n",
              "      <td>[-1.3856947, -1.3733647]</td>\n",
              "      <td>[3, 3]</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.save('workflow_etl')"
      ],
      "metadata": {
        "id": "49_24q2zvQl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sessions_gdf = sessions_gdf[sessions_gdf.day_index>=178]"
      ],
      "metadata": {
        "id": "iFvEfawTvQob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from merlin_standard_lib import Schema\n",
        "SCHEMA_PATH = \"/content/drive/MyDrive/Datasets/rsc15/raw/processed_nvt/schema.pbtxt\"\n",
        "schema = Schema().from_proto_text(SCHEMA_PATH)"
      ],
      "metadata": {
        "id": "OVr4R3gLvQrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = nvt.Dataset(interactions_merged_df)\n",
        "workflow = nvt.Workflow(filtered_sessions)\n",
        "# Learn features statistics necessary of the preprocessing workflow\n",
        "# The following will generate schema.pbtxt file in the provided folder and export the parquet files.\n",
        "workflow.fit_transform(dataset).to_parquet(os.path.join(DATA_FOLDER, \"processed_nvt\"))"
      ],
      "metadata": {
        "id": "Aar87EbCfKt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.output_schema"
      ],
      "metadata": {
        "id": "7Sd-C9uug4uO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "f7cfe761-203a-4a2e-85a5-5911ffcca3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'session_id', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': 'workflow_etl/categories/unique.session_id.parquet', 'domain': {'min': 0, 'max': 9249731, 'name': 'session_id'}, 'embedding_sizes': {'cardinality': 9249732, 'dimension': 512}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'item_id-count', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': 'workflow_etl/categories/unique.item_id.parquet', 'domain': {'min': 0, 'max': 52741, 'name': 'item_id'}, 'embedding_sizes': {'cardinality': 52742, 'dimension': 512}}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'item_id-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.LIST: 'list'>, <Tags.ID: 'id'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': 'workflow_etl/categories/unique.item_id.parquet', 'domain': {'min': 0, 'max': 52741, 'name': 'item_id'}, 'embedding_sizes': {'cardinality': 52742, 'dimension': 512}, 'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}, {'name': 'et_dayofweek_sin-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}, {'name': 'product_recency_days_log_norm-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='float32', element_type=<ElementType.Float: 'float'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}, {'name': 'category-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.LIST: 'list'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': 'workflow_etl/categories/unique.category.parquet', 'domain': {'min': 0, 'max': 336, 'name': 'category'}, 'embedding_sizes': {'cardinality': 337, 'dimension': 42}, 'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}, {'name': 'day_index', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>tags</th>\n",
              "      <th>dtype</th>\n",
              "      <th>is_list</th>\n",
              "      <th>is_ragged</th>\n",
              "      <th>properties.num_buckets</th>\n",
              "      <th>properties.freq_threshold</th>\n",
              "      <th>properties.max_size</th>\n",
              "      <th>properties.cat_path</th>\n",
              "      <th>properties.domain.min</th>\n",
              "      <th>properties.domain.max</th>\n",
              "      <th>properties.domain.name</th>\n",
              "      <th>properties.embedding_sizes.cardinality</th>\n",
              "      <th>properties.embedding_sizes.dimension</th>\n",
              "      <th>properties.value_count.min</th>\n",
              "      <th>properties.value_count.max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>session_id</td>\n",
              "      <td>(Tags.CATEGORICAL)</td>\n",
              "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>workflow_etl/categories/unique.session_id.parquet</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9249731.0</td>\n",
              "      <td>session_id</td>\n",
              "      <td>9249732.0</td>\n",
              "      <td>512.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>item_id-count</td>\n",
              "      <td>(Tags.CATEGORICAL)</td>\n",
              "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>workflow_etl/categories/unique.item_id.parquet</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52741.0</td>\n",
              "      <td>item_id</td>\n",
              "      <td>52742.0</td>\n",
              "      <td>512.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>item_id-list</td>\n",
              "      <td>(Tags.CATEGORICAL, Tags.LIST, Tags.ID, Tags.ITEM)</td>\n",
              "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>workflow_etl/categories/unique.item_id.parquet</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52741.0</td>\n",
              "      <td>item_id</td>\n",
              "      <td>52742.0</td>\n",
              "      <td>512.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>et_dayofweek_sin-list</td>\n",
              "      <td>(Tags.LIST, Tags.CONTINUOUS)</td>\n",
              "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>product_recency_days_log_norm-list</td>\n",
              "      <td>(Tags.LIST, Tags.CONTINUOUS)</td>\n",
              "      <td>DType(name='float32', element_type=&lt;ElementTyp...</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>category-list</td>\n",
              "      <td>(Tags.CATEGORICAL, Tags.LIST)</td>\n",
              "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>workflow_etl/categories/unique.category.parquet</td>\n",
              "      <td>0.0</td>\n",
              "      <td>336.0</td>\n",
              "      <td>category</td>\n",
              "      <td>337.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>day_index</td>\n",
              "      <td>(Tags.CATEGORICAL)</td>\n",
              "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.save(os.path.join(DATA_FOLDER, \"workflow_etl\"))"
      ],
      "metadata": {
        "id": "QDF78wbBq7NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the processed train dataset\n",
        "sessions_gdf = cudf.read_parquet(os.path.join(DATA_FOLDER, \"processed_nvt/part_0.parquet\"))\n",
        "if USE_SYNTHETIC:\n",
        "    THRESHOLD_DAY_INDEX = int(os.environ.get(\"THRESHOLD_DAY_INDEX\", '1'))\n",
        "    sessions_gdf = sessions_gdf[sessions_gdf.day_index>=THRESHOLD_DAY_INDEX]\n",
        "else:\n",
        "    sessions_gdf = sessions_gdf[sessions_gdf.day_index>=178]"
      ],
      "metadata": {
        "id": "OwFkql_bq-tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sessions_gdf.head(3))"
      ],
      "metadata": {
        "id": "BuSg4jljrPLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f84af5e2-67ae-4f32-8c00-1bc99672510f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         session_id  item_id-count  \\\n",
            "6606147    11255549             12   \n",
            "6606148    11255552              2   \n",
            "6606149    11255553              2   \n",
            "\n",
            "                                              item_id-list  \\\n",
            "6606147  [605, 879, 743, 91, 4778, 1584, 3447, 8084, 34...   \n",
            "6606148                                       [185, 12289]   \n",
            "6606149                                       [7300, 1954]   \n",
            "\n",
            "                                     et_dayofweek_sin-list  \\\n",
            "6606147  [-0.43388454782514785, -0.43388454782514785, -...   \n",
            "6606148       [-0.43388454782514785, -0.43388454782514785]   \n",
            "6606149         [-0.7818309228245777, -0.7818309228245777]   \n",
            "\n",
            "                        product_recency_days_log_norm-list  \\\n",
            "6606147  [1.5241561, 1.523876, 1.523935, 1.5241641, 1.5...   \n",
            "6606148                              [-0.533007, 1.521495]   \n",
            "6606149                             [1.5338274, 1.5355083]   \n",
            "\n",
            "                                category-list  day_index  \n",
            "6606147  [4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4]        178  \n",
            "6606148                                [1, 3]        178  \n",
            "6606149                                [8, 8]        180  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers4rec.utils.data_utils import save_time_based_splits\n",
        "save_time_based_splits(data=nvt.Dataset(sessions_gdf),\n",
        "                       output_dir=os.path.join(DATA_FOLDER, \"preproc_sessions_by_day\"),\n",
        "                       partition_col='day_index',\n",
        "                       timestamp_col='session_id',\n",
        "                      )"
      ],
      "metadata": {
        "id": "EurmlKoXrQr7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32697464-3b55-45a8-bb28-683f2b83404f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating time-based splits: 100%|██████████| 5/5 [00:02<00:00,  2.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# free gpu memory\n",
        "del  sessions_gdf\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "FBWO6UgxrSqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57dcfcce-e0fc-4a26-9dd0-3375eaaf7ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "845"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "end2end"
      ],
      "metadata": {
        "id": "p1Ptv2fd1NAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", \"/content/drive/MyDrive/Datasets/rsc15/raw\")\n",
        "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", f\"{INPUT_DATA_DIR}/preproc_sessions_by_day\")"
      ],
      "metadata": {
        "id": "ZIklcrburUlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from merlin.schema import Schema\n",
        "from merlin.io import Dataset\n",
        "\n",
        "train = Dataset(os.path.join(INPUT_DATA_DIR, \"processed_nvt/part_0.parquet\"))\n",
        "schema = train.schema"
      ],
      "metadata": {
        "id": "9AVmR4GpsFCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = schema.select_by_name(\n",
        "   ['item_id-list', 'category-list', 'product_recency_days_log_norm-list', 'et_dayofweek_sin-list']\n",
        ")"
      ],
      "metadata": {
        "id": "gF_rIVjmsK9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema"
      ],
      "metadata": {
        "id": "ZPkzuwkXsQJ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "1dacd286-b237-4565-a747-3f3262f2db8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'item_id-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.LIST: 'list'>, <Tags.ID: 'id'>, <Tags.ITEM: 'item'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': 'workflow_etl/categories/unique.item_id.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 512.0, 'cardinality': 52742.0}, 'domain': {'min': 0, 'max': 52741, 'name': 'item_id'}, 'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}, {'name': 'category-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.LIST: 'list'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': 'workflow_etl/categories/unique.category.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 42.0, 'cardinality': 337.0}, 'domain': {'min': 0, 'max': 336, 'name': 'category'}, 'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}, {'name': 'product_recency_days_log_norm-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='float32', element_type=<ElementType.Float: 'float'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}, {'name': 'et_dayofweek_sin-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>tags</th>\n",
              "      <th>dtype</th>\n",
              "      <th>is_list</th>\n",
              "      <th>is_ragged</th>\n",
              "      <th>properties.freq_threshold</th>\n",
              "      <th>properties.num_buckets</th>\n",
              "      <th>properties.cat_path</th>\n",
              "      <th>properties.max_size</th>\n",
              "      <th>properties.embedding_sizes.dimension</th>\n",
              "      <th>properties.embedding_sizes.cardinality</th>\n",
              "      <th>properties.domain.min</th>\n",
              "      <th>properties.domain.max</th>\n",
              "      <th>properties.domain.name</th>\n",
              "      <th>properties.value_count.min</th>\n",
              "      <th>properties.value_count.max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>item_id-list</td>\n",
              "      <td>(Tags.CATEGORICAL, Tags.LIST, Tags.ID, Tags.ITEM)</td>\n",
              "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>workflow_etl/categories/unique.item_id.parquet</td>\n",
              "      <td>0.0</td>\n",
              "      <td>512.0</td>\n",
              "      <td>52742.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52741.0</td>\n",
              "      <td>item_id</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>category-list</td>\n",
              "      <td>(Tags.CATEGORICAL, Tags.LIST)</td>\n",
              "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>workflow_etl/categories/unique.category.parquet</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>336.0</td>\n",
              "      <td>category</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>product_recency_days_log_norm-list</td>\n",
              "      <td>(Tags.LIST, Tags.CONTINUOUS)</td>\n",
              "      <td>DType(name='float32', element_type=&lt;ElementTyp...</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>et_dayofweek_sin-list</td>\n",
              "      <td>(Tags.LIST, Tags.CONTINUOUS)</td>\n",
              "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers4rec import torch as tr\n",
        "\n",
        "max_sequence_length, d_model = 20, 320\n",
        "# Define input module to process tabular input-features and to prepare masked inputs\n",
        "input_module = tr.TabularSequenceFeatures.from_schema(\n",
        "    schema,\n",
        "    max_sequence_length=max_sequence_length,\n",
        "    continuous_projection=64,\n",
        "    aggregation=\"concat\",\n",
        "    d_output=d_model,\n",
        "    masking=\"mlm\",\n",
        ")\n",
        "\n",
        "# Define Next item prediction-task\n",
        "prediction_task = tr.NextItemPredictionTask(weight_tying=True)\n",
        "\n",
        "# Define the config of the XLNet Transformer architecture\n",
        "transformer_config = tr.XLNetConfig.build(\n",
        "    d_model=d_model, n_head=8, n_layer=2, total_seq_length=max_sequence_length\n",
        ")\n",
        "\n",
        "# Get the end-to-end model\n",
        "model = transformer_config.to_torch_model(input_module, prediction_task)"
      ],
      "metadata": {
        "id": "Q8D-5nH9sRt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a78be7c-d74c-45b7-d5d1-7527f5e82cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers4rec:Projecting inputs of NextItemPredictionTask to'64' As weight tying requires the input dimension '320' to be equal to the item-id embedding dimension '64'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE_TRAIN = int(os.environ.get(\"BATCH_SIZE_TRAIN\", \"512\"))\n",
        "BATCH_SIZE_VALID = int(os.environ.get(\"BATCH_SIZE_VALID\", \"256\"))\n",
        "training_args = tr.trainer.T4RecTrainingArguments(\n",
        "            dataloader_drop_last = True,\n",
        "            gradient_accumulation_steps = 1,\n",
        "            output_dir=\"./tmp\",\n",
        "            max_sequence_length=20,\n",
        "            data_loader_engine='merlin',\n",
        "            num_train_epochs=10,\n",
        "            lr_scheduler_type='cosine',\n",
        "            # dataloader_drop_last=False,\n",
        "            per_device_train_batch_size = BATCH_SIZE_TRAIN,\n",
        "            per_device_eval_batch_size = BATCH_SIZE_VALID,\n",
        "            learning_rate=0.0005,\n",
        "            fp16=True,\n",
        "            report_to = [],\n",
        "            logging_steps=200\n",
        "        )"
      ],
      "metadata": {
        "id": "6-ai_BuBsT8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = tr.Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    schema=schema,\n",
        "    compute_metrics=True)"
      ],
      "metadata": {
        "id": "hXdPg-8csWmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QjYizlJYEpWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers4rec.torch.utils.examples_utils import fit_and_evaluate\n",
        "start_time_idx = int(os.environ.get(\"START_TIME_INDEX\", \"179\"))\n",
        "end_time_idx = int(os.environ.get(\"END_TIME_INDEX\", \"182\"))\n",
        "OT_results = fit_and_evaluate(recsys_trainer, start_time_index=start_time_idx, end_time_index=end_time_idx, input_dir=OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "2Fjs0DozsaEm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "outputId": "e83f7d03-6908-434d-cb39-2ceb21e2298f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "***** Launch training for day 179: *****\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='390' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [390/390 00:29, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>6.953600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers4rec/torch/trainer.py\u001b[0m in \u001b[0;36m_use_cuda_amp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;31m# Update the observed num examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'use_cuda_amp'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-8a5632bee0b7>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_time_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"START_TIME_INDEX\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"179\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mend_time_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"END_TIME_INDEX\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"182\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mOT_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecsys_trainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_time_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_time_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers4rec/torch/utils/examples_utils.py\u001b[0m in \u001b[0;36mfit_and_evaluate\u001b[0;34m(trainer, start_time_index, end_time_index, input_dir)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# 3. Evaluate on valid data of time_index+1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dataset_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0meval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n***** Evaluation results for day %s:*****\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtime_index_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3010\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3011\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   3012\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers4rec/torch/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m    500\u001b[0m                 \u001b[0;31m# Set back to None to begin a new accumulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m                 \u001b[0mlosses_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_item_ids_scores_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_index\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_past\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;31m# Clean the state at the end of the evaluation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers4rec/torch/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys, training, testing)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset must implement __len__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers4rec/torch/trainer.py\u001b[0m in \u001b[0;36m_use_cuda_amp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;31m# Update the observed num examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mobserved_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m                 \u001b[0mobserved_num_examples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mobserved_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'use_amp'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "multigpu"
      ],
      "metadata": {
        "id": "ONQJif5k1SSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINER_FILE = os.path.join(os.environ.get(\"INPUT_DATA_DIR\", \"/content/drive/MyDrive/Datasets/rsc15/raw\"), \"pyt_trainer.py\")"
      ],
      "metadata": {
        "id": "6GEnQDLm1WQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TRAINER_FILE}\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "\n",
        "import cupy\n",
        "\n",
        "from transformers4rec import torch as tr\n",
        "from transformers4rec.torch.ranking_metric import NDCGAt, AvgPrecisionAt, RecallAt\n",
        "from transformers4rec.torch.utils.examples_utils import wipe_memory\n",
        "from merlin.schema import Schema\n",
        "from merlin.io import Dataset\n",
        "\n",
        "cupy.cuda.Device(int(os.environ[\"LOCAL_RANK\"])).use()\n",
        "\n",
        "# define arguments that can be passed to this python script\n",
        "parser = argparse.ArgumentParser(description='Hyperparameters for model training')\n",
        "parser.add_argument('--path', type=str, help='Directory with training and validation data')\n",
        "parser.add_argument('--learning-rate', type=float, default=0.0005, help='Learning rate for training')\n",
        "parser.add_argument('--per-device-train-batch-size', type=int, default=64, help='Per device batch size for training')\n",
        "parser.add_argument('--per-device-eval-batch-size', type=int, default=32, help='Per device batch size for evaluation')\n",
        "sh_args = parser.parse_args()\n",
        "\n",
        "# create the schema object by reading the processed train set generated in the previous 01-ETL-with-NVTabular notebook\n",
        "\n",
        "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", \"/workspace/data\")\n",
        "train = Dataset(os.path.join(INPUT_DATA_DIR, \"processed_nvt/part_0.parquet\"))\n",
        "schema = train.schema\n",
        "\n",
        "# select the subset of features we want to use for training the model by their tags or their names.\n",
        "schema = schema.select_by_name(\n",
        "   ['item_id-list', 'category-list', 'product_recency_days_log_norm-list', 'et_dayofweek_sin-list']\n",
        ")\n",
        "\n",
        "max_sequence_length, d_model = 20, 320\n",
        "# Define input module to process tabular input-features and to prepare masked inputs\n",
        "input_module = tr.TabularSequenceFeatures.from_schema(\n",
        "    schema,\n",
        "    max_sequence_length=max_sequence_length,\n",
        "    continuous_projection=64,\n",
        "    aggregation=\"concat\",\n",
        "    d_output=d_model,\n",
        "    masking=\"mlm\",\n",
        ")\n",
        "\n",
        "# Define Next item prediction-task\n",
        "prediction_task = tr.NextItemPredictionTask(weight_tying=True)\n",
        "\n",
        "# Define the config of the XLNet Transformer architecture\n",
        "transformer_config = tr.XLNetConfig.build(\n",
        "    d_model=d_model, n_head=8, n_layer=2, total_seq_length=max_sequence_length\n",
        ")\n",
        "\n",
        "# Get the end-to-end model\n",
        "model = transformer_config.to_torch_model(input_module, prediction_task)\n",
        "\n",
        "# Set training arguments\n",
        "training_args = tr.trainer.T4RecTrainingArguments(\n",
        "            output_dir=\"./tmp\",\n",
        "            max_sequence_length=20,\n",
        "            data_loader_engine='merlin',\n",
        "            num_train_epochs=10,\n",
        "            dataloader_drop_last=True,\n",
        "            per_device_train_batch_size = sh_args.per_device_train_batch_size,\n",
        "            per_device_eval_batch_size = sh_args.per_device_eval_batch_size,\n",
        "            learning_rate=sh_args.learning_rate,\n",
        "            report_to = [],\n",
        "            logging_steps=200,\n",
        "        )\n",
        "\n",
        "# Instantiate the trainer\n",
        "recsys_trainer = tr.Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    schema=schema,\n",
        "    compute_metrics=True)\n",
        "\n",
        "# Set input and output directories\n",
        "INPUT_DIR=sh_args.path\n",
        "OUTPUT_DIR=sh_args.path\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# main loop for training\n",
        "start_time_window_index = int(os.environ.get(\"START_TIME_INDEX\", \"178\"))\n",
        "final_time_window_index = int(os.environ.get(\"END_TIME_INDEX\", \"181\"))\n",
        "\n",
        "# Iterating over days from 178 to 181\n",
        "for time_index in range(start_time_window_index, final_time_window_index):\n",
        "    # Set data\n",
        "    time_index_train = time_index\n",
        "    time_index_eval = time_index + 1\n",
        "    train_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_train}/train.parquet\"))\n",
        "    eval_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_eval}/valid.parquet\"))\n",
        "\n",
        "    # Train on day related to time_index\n",
        "    print('*'*20)\n",
        "    print(\"Launch training for day %s are:\" %time_index)\n",
        "    print('*'*20 + '\\n')\n",
        "    recsys_trainer.train_dataset_or_path = train_paths\n",
        "    recsys_trainer.reset_lr_scheduler()\n",
        "    recsys_trainer.train()\n",
        "    recsys_trainer.state.global_step +=1\n",
        "    print('finished')\n",
        "\n",
        "    # Evaluate on the following day\n",
        "    recsys_trainer.eval_dataset_or_path = eval_paths\n",
        "    eval_metrics = recsys_trainer.evaluate(metric_key_prefix='eval')\n",
        "    print('*'*20)\n",
        "    print(\"Eval results for day %s are:\\t\" %time_index_eval)\n",
        "    print('\\n' + '*'*20 + '\\n')\n",
        "    for key in sorted(eval_metrics.keys()):\n",
        "        print(\" %s = %s\" % (key, str(eval_metrics[key])))\n",
        "    wipe_memory()\n",
        "\n",
        "# export evaluation metrics to a file\n",
        "import json\n",
        "fname = os.path.join(INPUT_DATA_DIR, \"eval_metrics.txt\")\n",
        "f = open(fname, \"w\")\n",
        "f.write(json.dumps(eval_metrics))\n",
        "f.close()\n",
        "\n",
        "end = time.time()\n",
        "print('Total training time:', end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CBGUru11Yqh",
        "outputId": "82e212d9-a560-4648-b4c8-22c0bdf37721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/Datasets/rsc15/raw/pyt_trainer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda import device_count\n",
        "num_gpus = device_count()\n",
        "NUM_PROCESSES = min(num_gpus, 2)"
      ],
      "metadata": {
        "id": "dGAgZrcs1frS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", f\"{DATA_FOLDER}/preproc_sessions_by_day\")\n",
        "LR = float(os.environ.get(\"LEARNING_RATE\", \"0.0005\"))\n",
        "BATCH_SIZE_TRAIN = int(os.environ.get(\"BATCH_SIZE_TRAIN\", \"256\"))\n",
        "BATCH_SIZE_VALID = int(os.environ.get(\"BATCH_SIZE_VALID\", \"128\"))"
      ],
      "metadata": {
        "id": "wNJCwMX21lCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "IN9s1msZ3bgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.run --nproc_per_node {NUM_PROCESSES} {TRAINER_FILE} --path {OUTPUT_DIR} --learning-rate {LR} --per-device-train-batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7u62Jvi1vWv",
        "outputId": "61d70125-b853-4d2a-cecb-4cf49f6823cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-12 18:52:06.121409: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 18:52:06.121469: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 18:52:06.121508: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 18:52:11.708901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "usage: pyt_trainer.py [-h] [--path PATH] [--learning-rate LEARNING_RATE]\n",
            "                      [--per-device-train-batch-size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
            "                      [--per-device-eval-batch-size PER_DEVICE_EVAL_BATCH_SIZE]\n",
            "pyt_trainer.py: error: argument --per-device-train-batch-size: expected one argument\n",
            "[2023-11-12 18:52:27,580] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 2) local_rank: 0 (pid: 2943) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 810, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 806, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 797, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "/content/drive/MyDrive/Datasets/rsc15/raw/pyt_trainer.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-11-12_18:52:27\n",
            "  host      : 3dc16f114f88\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 2 (pid: 2943)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9yG3Y5Vt12gl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}